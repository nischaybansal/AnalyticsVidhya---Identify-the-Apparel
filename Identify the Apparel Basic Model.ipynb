{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "### About Practice Problem : Identify the Apparels\n",
    "\n",
    "More than 25% of entire revenue in E-Commerce is attributed to apparels & accessories. A major problem they face is categorizing these apparels from just the images especially when the categories provided by the brands are inconsistent. This poses an interesting computer vision problem which has caught the eyes of several deep learning researchers.\n",
    "Fashion MNIST is a drop-in replacement for the very well known, machine learning hello world - MNIST dataset which can be checked out at ‘Identify the digits’ practice problem. Instead of digits, the images show a type of apparel e.g. T-shirt, trousers, bag, etc. The dataset used in this problem was created by Zalando Research. More details can be found at this link.\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "We have total 70,000 images (28 x 28), out of which 60,000 are part of train images with the label of the type of apparel (total classes: 10) and rest 10,000 images are unlabelled (known as test images).The task is to identify the type of apparel for all test images. Given below is the code description for each of the apparel class/label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import the libraries\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label\n",
       "0   1      9\n",
       "1   2      0\n",
       "2   3      0\n",
       "3   4      3\n",
       "4   5      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Readinf our train dataset\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60000 [00:00<?, ?it/s]/anaconda3/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:104: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n",
      "100%|██████████| 60000/60000 [00:18<00:00, 3169.17it/s]\n"
     ]
    }
   ],
   "source": [
    "#we will read all the training images, store them in a list, and finally convert that list into a numpy array\n",
    "'''\n",
    "# We have grayscale images, so while loading the images we will keep grayscale=True, if you have RGB images, you should set grayscale as False\n",
    "train_image = []\n",
    "for i in tqdm(range(train.shape[0])):\n",
    "    img = image.load_img('train/'+train['id'][i].astype('str')+'.png', target_size=(28,28,1), grayscale=True)\n",
    "    img = image.img_to_array(img)\n",
    "    img = img/255\n",
    "    train_image.append(img)\n",
    "X = np.array(train_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60000/60000 [02:02<00:00, 489.16it/s]\n"
     ]
    }
   ],
   "source": [
    "#we will read all the training images, store them in a list, and finally convert that list into a numpy array\n",
    "\n",
    "# We have grayscale images, so while loading the images we will keep grayscale=True, if you have RGB images, you should set grayscale as False\n",
    "train_image = []\n",
    "for i in tqdm(range(train.shape[0])):\n",
    "    img = image.load_img('train/'+train['id'][i].astype('str')+'.png', target_size=(28,28,1), grayscale=True)\n",
    "    img = image.img_to_array(img)\n",
    "    img = img/255\n",
    "    train_image.append(img)\n",
    "X = np.array(train_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode the target variable.\n",
    "\n",
    "y=train['label'].values\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into train and validation \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple architecture with 2 convolutional layers, one dense hidden layer and an output layer.\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=(28,28,1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Trying to increase output by applying progressive resizing\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, kernel_size=(3, 3),activation='relu',input_shape=(48,48,1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple architecture with 2 convolutional layers, one dense hidden layer and an output layer.\n",
    "\n",
    "model1 = Sequential()\n",
    "\n",
    "model1.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=(28,28,1)))\n",
    "model1.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(Dropout(0.25))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(128, activation='relu'))\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "# Complie and fit \n",
    "\n",
    "model1.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complie and fit \n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Using RMS prop for the same 28 size \n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "model.compile(optimizer=RMSprop(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Compiling using RMS prop\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(optimizer=RMSprop(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/30\n",
      "48000/48000 [==============================] - 143s 3ms/step - loss: 0.1391 - accuracy: 0.9475 - val_loss: 0.2335 - val_accuracy: 0.9274\n",
      "Epoch 2/30\n",
      "48000/48000 [==============================] - 147s 3ms/step - loss: 0.1319 - accuracy: 0.9506 - val_loss: 0.2500 - val_accuracy: 0.9236\n",
      "Epoch 3/30\n",
      "48000/48000 [==============================] - 161s 3ms/step - loss: 0.1264 - accuracy: 0.9526 - val_loss: 0.2275 - val_accuracy: 0.9294\n",
      "Epoch 4/30\n",
      "48000/48000 [==============================] - 161s 3ms/step - loss: 0.1168 - accuracy: 0.9556 - val_loss: 0.2538 - val_accuracy: 0.9303\n",
      "Epoch 5/30\n",
      "48000/48000 [==============================] - 160s 3ms/step - loss: 0.1138 - accuracy: 0.9574 - val_loss: 0.2572 - val_accuracy: 0.9242\n",
      "Epoch 6/30\n",
      "48000/48000 [==============================] - 160s 3ms/step - loss: 0.1101 - accuracy: 0.9581 - val_loss: 0.2536 - val_accuracy: 0.9287\n",
      "Epoch 7/30\n",
      "48000/48000 [==============================] - 159s 3ms/step - loss: 0.1083 - accuracy: 0.9591 - val_loss: 0.2526 - val_accuracy: 0.9292\n",
      "Epoch 8/30\n",
      "48000/48000 [==============================] - 160s 3ms/step - loss: 0.1021 - accuracy: 0.9615 - val_loss: 0.2566 - val_accuracy: 0.9327\n",
      "Epoch 9/30\n",
      "48000/48000 [==============================] - 159s 3ms/step - loss: 0.0949 - accuracy: 0.9643 - val_loss: 0.2846 - val_accuracy: 0.9286\n",
      "Epoch 10/30\n",
      "48000/48000 [==============================] - 160s 3ms/step - loss: 0.0934 - accuracy: 0.9644 - val_loss: 0.2623 - val_accuracy: 0.9273\n",
      "Epoch 11/30\n",
      "48000/48000 [==============================] - 159s 3ms/step - loss: 0.0944 - accuracy: 0.9647 - val_loss: 0.2815 - val_accuracy: 0.9296\n",
      "Epoch 12/30\n",
      "48000/48000 [==============================] - 161s 3ms/step - loss: 0.0909 - accuracy: 0.9667 - val_loss: 0.2732 - val_accuracy: 0.9258\n",
      "Epoch 13/30\n",
      "48000/48000 [==============================] - 161s 3ms/step - loss: 0.0843 - accuracy: 0.9688 - val_loss: 0.3070 - val_accuracy: 0.9278\n",
      "Epoch 14/30\n",
      "48000/48000 [==============================] - 147s 3ms/step - loss: 0.0853 - accuracy: 0.9679 - val_loss: 0.3035 - val_accuracy: 0.9301\n",
      "Epoch 15/30\n",
      "48000/48000 [==============================] - 147s 3ms/step - loss: 0.0827 - accuracy: 0.9692 - val_loss: 0.3057 - val_accuracy: 0.9312\n",
      "Epoch 16/30\n",
      "48000/48000 [==============================] - 142s 3ms/step - loss: 0.0837 - accuracy: 0.9689 - val_loss: 0.2882 - val_accuracy: 0.9280\n",
      "Epoch 17/30\n",
      "48000/48000 [==============================] - 141s 3ms/step - loss: 0.0802 - accuracy: 0.9698 - val_loss: 0.2907 - val_accuracy: 0.9293\n",
      "Epoch 18/30\n",
      "48000/48000 [==============================] - 147s 3ms/step - loss: 0.0777 - accuracy: 0.9711 - val_loss: 0.2968 - val_accuracy: 0.9290\n",
      "Epoch 19/30\n",
      "48000/48000 [==============================] - 150s 3ms/step - loss: 0.0774 - accuracy: 0.9715 - val_loss: 0.3258 - val_accuracy: 0.9316\n",
      "Epoch 20/30\n",
      "48000/48000 [==============================] - 155s 3ms/step - loss: 0.0729 - accuracy: 0.9721 - val_loss: 0.3082 - val_accuracy: 0.9258\n",
      "Epoch 21/30\n",
      "48000/48000 [==============================] - 151s 3ms/step - loss: 0.0740 - accuracy: 0.9716 - val_loss: 0.3148 - val_accuracy: 0.9297\n",
      "Epoch 22/30\n",
      "48000/48000 [==============================] - 143s 3ms/step - loss: 0.0742 - accuracy: 0.9730 - val_loss: 0.2931 - val_accuracy: 0.9309\n",
      "Epoch 23/30\n",
      "48000/48000 [==============================] - 144s 3ms/step - loss: 0.0713 - accuracy: 0.9727 - val_loss: 0.3107 - val_accuracy: 0.9311\n",
      "Epoch 24/30\n",
      "48000/48000 [==============================] - 146s 3ms/step - loss: 0.0699 - accuracy: 0.9737 - val_loss: 0.3142 - val_accuracy: 0.9279\n",
      "Epoch 25/30\n",
      "48000/48000 [==============================] - 146s 3ms/step - loss: 0.0709 - accuracy: 0.9738 - val_loss: 0.3387 - val_accuracy: 0.9288\n",
      "Epoch 26/30\n",
      "48000/48000 [==============================] - 148s 3ms/step - loss: 0.0695 - accuracy: 0.9740 - val_loss: 0.3508 - val_accuracy: 0.9279\n",
      "Epoch 27/30\n",
      "48000/48000 [==============================] - 143s 3ms/step - loss: 0.0637 - accuracy: 0.9761 - val_loss: 0.3417 - val_accuracy: 0.9312\n",
      "Epoch 28/30\n",
      "48000/48000 [==============================] - 144s 3ms/step - loss: 0.0672 - accuracy: 0.9754 - val_loss: 0.3282 - val_accuracy: 0.9258\n",
      "Epoch 29/30\n",
      "48000/48000 [==============================] - 143s 3ms/step - loss: 0.0669 - accuracy: 0.9754 - val_loss: 0.3361 - val_accuracy: 0.9307\n",
      "Epoch 30/30\n",
      "48000/48000 [==============================] - 144s 3ms/step - loss: 0.0652 - accuracy: 0.9762 - val_loss: 0.3310 - val_accuracy: 0.9312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a4304d978>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(X_train, y_train, epochs=30, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/40\n",
      "48000/48000 [==============================] - 142s 3ms/step - loss: 0.0954 - accuracy: 0.9639 - val_loss: 0.2705 - val_accuracy: 0.9275\n",
      "Epoch 2/40\n",
      "48000/48000 [==============================] - 140s 3ms/step - loss: 0.0923 - accuracy: 0.9653 - val_loss: 0.2705 - val_accuracy: 0.9258\n",
      "Epoch 3/40\n",
      "48000/48000 [==============================] - 139s 3ms/step - loss: 0.0887 - accuracy: 0.9661 - val_loss: 0.2658 - val_accuracy: 0.9282\n",
      "Epoch 4/40\n",
      "48000/48000 [==============================] - 137s 3ms/step - loss: 0.0865 - accuracy: 0.9668 - val_loss: 0.2679 - val_accuracy: 0.9314\n",
      "Epoch 5/40\n",
      "48000/48000 [==============================] - 139s 3ms/step - loss: 0.0877 - accuracy: 0.9686 - val_loss: 0.2729 - val_accuracy: 0.9288\n",
      "Epoch 6/40\n",
      "48000/48000 [==============================] - 139s 3ms/step - loss: 0.0866 - accuracy: 0.9677 - val_loss: 0.2687 - val_accuracy: 0.9294\n",
      "Epoch 7/40\n",
      "48000/48000 [==============================] - 140s 3ms/step - loss: 0.0816 - accuracy: 0.9691 - val_loss: 0.2917 - val_accuracy: 0.9287\n",
      "Epoch 8/40\n",
      "48000/48000 [==============================] - 139s 3ms/step - loss: 0.0817 - accuracy: 0.9693 - val_loss: 0.2699 - val_accuracy: 0.9298\n",
      "Epoch 9/40\n",
      "48000/48000 [==============================] - 140s 3ms/step - loss: 0.0770 - accuracy: 0.9713 - val_loss: 0.2966 - val_accuracy: 0.9309\n",
      "Epoch 10/40\n",
      "48000/48000 [==============================] - 141s 3ms/step - loss: 0.0765 - accuracy: 0.9715 - val_loss: 0.2666 - val_accuracy: 0.9306\n",
      "Epoch 11/40\n",
      "48000/48000 [==============================] - 140s 3ms/step - loss: 0.0781 - accuracy: 0.9712 - val_loss: 0.2851 - val_accuracy: 0.9298\n",
      "Epoch 12/40\n",
      "48000/48000 [==============================] - 140s 3ms/step - loss: 0.0758 - accuracy: 0.9726 - val_loss: 0.2913 - val_accuracy: 0.9270\n",
      "Epoch 13/40\n",
      "48000/48000 [==============================] - 141s 3ms/step - loss: 0.0723 - accuracy: 0.9734 - val_loss: 0.2948 - val_accuracy: 0.9301\n",
      "Epoch 14/40\n",
      "48000/48000 [==============================] - 141s 3ms/step - loss: 0.0736 - accuracy: 0.9737 - val_loss: 0.3101 - val_accuracy: 0.9319\n",
      "Epoch 15/40\n",
      "48000/48000 [==============================] - 141s 3ms/step - loss: 0.0718 - accuracy: 0.9736 - val_loss: 0.3126 - val_accuracy: 0.9274\n",
      "Epoch 16/40\n",
      "48000/48000 [==============================] - 142s 3ms/step - loss: 0.0687 - accuracy: 0.9744 - val_loss: 0.3307 - val_accuracy: 0.9301\n",
      "Epoch 17/40\n",
      "48000/48000 [==============================] - 141s 3ms/step - loss: 0.0689 - accuracy: 0.9751 - val_loss: 0.3553 - val_accuracy: 0.9287\n",
      "Epoch 18/40\n",
      "48000/48000 [==============================] - 146s 3ms/step - loss: 0.0693 - accuracy: 0.9743 - val_loss: 0.2839 - val_accuracy: 0.9312\n",
      "Epoch 19/40\n",
      "48000/48000 [==============================] - 141s 3ms/step - loss: 0.0681 - accuracy: 0.9753 - val_loss: 0.2968 - val_accuracy: 0.9282\n",
      "Epoch 20/40\n",
      "48000/48000 [==============================] - 142s 3ms/step - loss: 0.0696 - accuracy: 0.9746 - val_loss: 0.3091 - val_accuracy: 0.9302\n",
      "Epoch 21/40\n",
      "48000/48000 [==============================] - 143s 3ms/step - loss: 0.0612 - accuracy: 0.9775 - val_loss: 0.3595 - val_accuracy: 0.9328\n",
      "Epoch 22/40\n",
      "48000/48000 [==============================] - 142s 3ms/step - loss: 0.0640 - accuracy: 0.9769 - val_loss: 0.3144 - val_accuracy: 0.9307\n",
      "Epoch 23/40\n",
      "48000/48000 [==============================] - 142s 3ms/step - loss: 0.0672 - accuracy: 0.9755 - val_loss: 0.3145 - val_accuracy: 0.9321\n",
      "Epoch 24/40\n",
      "48000/48000 [==============================] - 153s 3ms/step - loss: 0.0645 - accuracy: 0.9769 - val_loss: 0.3081 - val_accuracy: 0.9319\n",
      "Epoch 25/40\n",
      "48000/48000 [==============================] - 145s 3ms/step - loss: 0.0630 - accuracy: 0.9780 - val_loss: 0.3508 - val_accuracy: 0.9340\n",
      "Epoch 26/40\n",
      "48000/48000 [==============================] - 150s 3ms/step - loss: 0.0636 - accuracy: 0.9759 - val_loss: 0.3117 - val_accuracy: 0.9296\n",
      "Epoch 27/40\n",
      "48000/48000 [==============================] - 147s 3ms/step - loss: 0.0607 - accuracy: 0.9777 - val_loss: 0.3362 - val_accuracy: 0.9295\n",
      "Epoch 28/40\n",
      "48000/48000 [==============================] - 143s 3ms/step - loss: 0.0627 - accuracy: 0.9771 - val_loss: 0.3168 - val_accuracy: 0.9314\n",
      "Epoch 29/40\n",
      "48000/48000 [==============================] - 144s 3ms/step - loss: 0.0615 - accuracy: 0.9783 - val_loss: 0.3820 - val_accuracy: 0.9303\n",
      "Epoch 30/40\n",
      "48000/48000 [==============================] - 143s 3ms/step - loss: 0.0614 - accuracy: 0.9777 - val_loss: 0.3574 - val_accuracy: 0.9323\n",
      "Epoch 31/40\n",
      "48000/48000 [==============================] - 144s 3ms/step - loss: 0.0594 - accuracy: 0.9783 - val_loss: 0.3529 - val_accuracy: 0.9319\n",
      "Epoch 32/40\n",
      "48000/48000 [==============================] - 144s 3ms/step - loss: 0.0627 - accuracy: 0.9776 - val_loss: 0.3876 - val_accuracy: 0.9298\n",
      "Epoch 33/40\n",
      "48000/48000 [==============================] - 153s 3ms/step - loss: 0.0561 - accuracy: 0.9797 - val_loss: 0.3487 - val_accuracy: 0.9311\n",
      "Epoch 34/40\n",
      "48000/48000 [==============================] - 149s 3ms/step - loss: 0.0592 - accuracy: 0.9795 - val_loss: 0.3664 - val_accuracy: 0.9319\n",
      "Epoch 35/40\n",
      "48000/48000 [==============================] - 149s 3ms/step - loss: 0.0614 - accuracy: 0.9792 - val_loss: 0.3541 - val_accuracy: 0.9301\n",
      "Epoch 36/40\n",
      "48000/48000 [==============================] - 141s 3ms/step - loss: 0.0552 - accuracy: 0.9800 - val_loss: 0.3523 - val_accuracy: 0.9313\n",
      "Epoch 37/40\n",
      "48000/48000 [==============================] - 147s 3ms/step - loss: 0.0587 - accuracy: 0.9784 - val_loss: 0.3404 - val_accuracy: 0.9327\n",
      "Epoch 38/40\n",
      "48000/48000 [==============================] - 143s 3ms/step - loss: 0.0545 - accuracy: 0.9812 - val_loss: 0.3396 - val_accuracy: 0.9308\n",
      "Epoch 39/40\n",
      "48000/48000 [==============================] - 141s 3ms/step - loss: 0.0584 - accuracy: 0.9794 - val_loss: 0.3852 - val_accuracy: 0.9316\n",
      "Epoch 40/40\n",
      "48000/48000 [==============================] - 142s 3ms/step - loss: 0.0546 - accuracy: 0.9803 - val_loss: 0.3528 - val_accuracy: 0.9308\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a4303b8d0>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(X_train, y_train, epochs=40, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]/anaconda3/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:104: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n",
      "100%|██████████| 10000/10000 [00:20<00:00, 491.76it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_image = []\n",
    "for i in tqdm(range(test.shape[0])):\n",
    "    img = image.load_img('test/'+test['id'][i].astype('str')+'.png', target_size=(28,28,1), grayscale=True)\n",
    "    img = image.img_to_array(img)\n",
    "    img = img/255\n",
    "    test_image.append(img)\n",
    "test = np.array(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# making predictions\n",
    "prediction = model.predict_classes(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# making predictions\n",
    "predictionInput = model.predict_classes(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making predictions\n",
    "predictionInput30Adam28 = model.predict_classes(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making predictions\n",
    "predictionInput40Adam28EP20 = model1.predict_classes(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# creating submission file\n",
    "sample = pd.read_csv('sample_submission.csv')\n",
    "sample['label'] = predictionInput30Adam28\n",
    "sample.to_csv('sample_cnn30Adam28.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# creating submission file\n",
    "sample = pd.read_csv('sample_submission.csv')\n",
    "sample['label'] = predictionInput40Adam28EP20\n",
    "sample.to_csv('sample_cnn40Adam28EP20.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
